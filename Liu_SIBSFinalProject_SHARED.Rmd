---
title: "Liu_SIBSHackathon"
author: "Lucy Liu"
date: "7/10/2022"
output: html_document
---

## Stage 1: Background & Research Questions
### Load in data
```{r}
library(tidyverse)
mi_comp <- read.csv('D:/NCSU/Summer2022/SIBS/SIBS_HackAThon/Myocardial infarction complications Database.csv', check.names = TRUE) #make sure the variable imported valid for R without special characters/symbols like tabs

#TRY
```

```{r}
summary(mi_comp)
#when doing lm(), ADD na.action=na.omit --> exclude from the analysis any subject who does not have all the variables necessary to fit a model
#Logistic regression model would work better since a lot of categorical var.
```
```{r}
#head(mi_comp)
#tail(mi_comp)
str(mi_comp) #Display the structure of a dataset. Gives data type, dimensions, column names, and data type within columns

###need to change to correct type???????? Some categorical some binary
#mi_compNEW <- for(i in 49:85){
#  as.factor(mi_comp)
#}

#mi_compNEW <- as.binary()

#Makecorrect variable type (categorical)

#################Ordinal attribute to NUMERICAL so can look at Odds ratio################
Cols <- c("ID", "SEX",	"IBS_NASL", "SIM_GIPERT",	"ZSN_A",	"nr_11", "nr_01",	"nr_02",	"nr_03",	"nr_04",	"nr_07",	"nr_08",	"np_01",	"np_04",	"np_05",	"np_07",	"np_08"	,"np_09",	"np_10"	,	"endocr_02",	"endocr_03",	"zab_leg_01",	"zab_leg_02",	"zab_leg_03",	"zab_leg_04",	"zab_leg_06",		"O_L_POST",	"K_SH_POST",	"MP_TP_POST",	"SVT_POST",	"GT_POST",	"FIB_G_POST",		"IM_PG_P",	"ritm_ecg_p_01",	"ritm_ecg_p_02",	"ritm_ecg_p_04",	"ritm_ecg_p_06",	"ritm_ecg_p_07",	"ritm_ecg_p_08",	"n_r_ecg_p_01",	"n_r_ecg_p_02",	"n_r_ecg_p_03",	"n_r_ecg_p_04",	"n_r_ecg_p_05",	"n_r_ecg_p_06",	"n_r_ecg_p_08",	"n_r_ecg_p_09",	"n_r_ecg_p_10",	"n_p_ecg_p_01",	"n_p_ecg_p_03",	"n_p_ecg_p_04",	"n_p_ecg_p_05",	"n_p_ecg_p_06",	"n_p_ecg_p_07",	"n_p_ecg_p_08",	"n_p_ecg_p_09",	"n_p_ecg_p_10",	"n_p_ecg_p_11",	"n_p_ecg_p_12",	"fibr_ter_01",	"fibr_ter_02",	"fibr_ter_03",	"fibr_ter_05",	"fibr_ter_06",	"fibr_ter_07",	"fibr_ter_08",	"GIPO_K",		"GIPER_NA",	"NA_KB",	"NOT_NA_KB",	"LID_KB",	"NITR_S",	"NOT_NA_1_n",	"LID_S_n",	"B_BLOK_S_n",	"ANT_CA_S_n",	"GEPAR_S_n",	"ASP_S_n",	"TIKL_S_n",	"TRENT_S_n",	"FIBR_PREDS",	"PREDS_TAH",	"JELUD_TAH",	"FIBR_JELUD",	"A_V_BLOK",	"OTEK_LANC",	"RAZRIV",	"DRESSLER",	"ZSN",	"REC_IM",	"P_IM_STEN",	"LET_IS")

#################Ordinal attribute to CATEGORICAL so can look at Odds ratio################
Cols <- c("ID", "SEX",	"IBS_ANAM",  "STENOK_AN", "FK_STENOK", "IBS_POST","IBS_NASL", "GB","SIM_GIPERT","DLIT_AG",	"ZSN_A",	"nr_11", "nr_01",	"nr_02",	"nr_03",	"nr_04",	"nr_07",	"nr_08",	"np_01",	"np_04",	"np_05",	"np_07",	"np_08"	,"np_09",	"np_10"	,	"endocr_02",	"endocr_03",	"zab_leg_01",	"zab_leg_02",	"zab_leg_03",	"zab_leg_04",	"zab_leg_06",		"O_L_POST",	"K_SH_POST",	"MP_TP_POST",	"SVT_POST",	"GT_POST",	"FIB_G_POST",	"ant_im",	"lat_im","inf_im","post_im","IM_PG_P",	"ritm_ecg_p_01",	"ritm_ecg_p_02",	"ritm_ecg_p_04",	"ritm_ecg_p_06",	"ritm_ecg_p_07",	"ritm_ecg_p_08",	"n_r_ecg_p_01",	"n_r_ecg_p_02",	"n_r_ecg_p_03",	"n_r_ecg_p_04",	"n_r_ecg_p_05",	"n_r_ecg_p_06",	"n_r_ecg_p_08",	"n_r_ecg_p_09",	"n_r_ecg_p_10",	"n_p_ecg_p_01",	"n_p_ecg_p_03",	"n_p_ecg_p_04",	"n_p_ecg_p_05",	"n_p_ecg_p_06",	"n_p_ecg_p_07",	"n_p_ecg_p_08",	"n_p_ecg_p_09",	"n_p_ecg_p_10",	"n_p_ecg_p_11",	"n_p_ecg_p_12",	"fibr_ter_01",	"fibr_ter_02",	"fibr_ter_03",	"fibr_ter_05",	"fibr_ter_06",	"fibr_ter_07",	"fibr_ter_08",	"GIPO_K",		"GIPER_NA",	"TIME_B_S","NA_KB",	"NOT_NA_KB",	"LID_KB",	"NITR_S",	"NOT_NA_1_n",	"LID_S_n",	"B_BLOK_S_n",	"ANT_CA_S_n",	"GEPAR_S_n",	"ASP_S_n",	"TIKL_S_n",	"TRENT_S_n",	"FIBR_PREDS",	"PREDS_TAH",	"JELUD_TAH",	"FIBR_JELUD",	"A_V_BLOK",	"OTEK_LANC",	"RAZRIV",	"DRESSLER",	"ZSN",	"REC_IM",	"P_IM_STEN",	"LET_IS")

###########HOW TO CHECK IF VARIABLE ARE SPELLED RIGHT##################
#Cols %in% names(mi_comp)
#Cols[Cols %in% names(mi_comp)==FALSE] --> to get which one that is wrong 
##########################################################################

#namesNEW <- function(data){
#  mi_comp$data <- as.factor(mi_comp$data)
#}


#mi_compNEW <- for (i in names){
#  namesNEW(i)
#}
mi_compNEW = mi_comp

for (i in Cols){
  mi_compNEW[,i] <- as.factor(mi_comp[,i])
}

#mi_comp[,names] <- as.factor(mi_comp[, names])
#Error in `$<-.data.frame`(`*tmp*`, data, value = integer(0)) : 
#  replacement has 0 rows, data has 1700


#variable that can be numerical OR cumulative dummy coding:
#INF_ANAM	STENOK_AN FK_STENOK IBS_POST GB DLIT_AG 
#ant_im	lat_im	inf_im	post_im TIME_B_S
####Possible source of error!!! Ordinary attribute are not accurate, only shows order!!!

#numerical variables:
#S_AD_KBRIG	D_AD_KBRIG	S_AD_ORIT	D_AD_ORIT K_BLOOD NA_BLOOD	ALT_BLOOD AST_BLOOD	KFK_BLOOD	L_BLOOD	ROE
#######mi_comp[names] <- lapply(mi_comp[names], factor)
str(mi_compNEW)

```
- Still need more understanding of the variables.
- Assume all explanatory/covariates measured after 3rd day of hospitalization.
- Need to change variables to easier writing. Ex. NOT mi_comp$age --> age. Since there are 124 variables (111 covariates, 12 complications, 1 ID), need a FOR LOOP to do that? ANSWER: NOPE! in lm(), just add data=mi_comp option!
- Change variable to correct type so analysis.
- Maybe would help to create own variable?
```{r}
#ID	AGE	SEX	INF_ANAM	STENOK_AN	FK_STENOK	IBS_POST	IBS_NASL	GB	SIM_GIPERT	DLIT_AG	ZSN_A	nr_11	nr_01	nr_02	nr_03	nr_04	nr_07	nr_08	np_01	np_04	np_05	np_07	np_08	np_09	np_10	endocr_01	endocr_02	endocr_03	zab_leg_01	zab_leg_02	zab_leg_03	zab_leg_04	zab_leg_06	S_AD_KBRIG	D_AD_KBRIG	S_AD_ORIT	D_AD_ORIT	O_L_POST	K_SH_POST	MP_TP_POST	SVT_POST	GT_POST	FIB_G_POST	ant_im	lat_im	inf_im	post_im	IM_PG_P	ritm_ecg_p_01	ritm_ecg_p_02	ritm_ecg_p_04	ritm_ecg_p_06	ritm_ecg_p_07	ritm_ecg_p_08	n_r_ecg_p_01	n_r_ecg_p_02	n_r_ecg_p_03	n_r_ecg_p_04	n_r_ecg_p_05	n_r_ecg_p_06	n_r_ecg_p_08	n_r_ecg_p_09	n_r_ecg_p_10	n_p_ecg_p_01	n_p_ecg_p_03	n_p_ecg_p_04	n_p_ecg_p_05	n_p_ecg_p_06	n_p_ecg_p_07	n_p_ecg_p_08	n_p_ecg_p_09	n_p_ecg_p_10	n_p_ecg_p_11	n_p_ecg_p_12	fibr_ter_01	fibr_ter_02	fibr_ter_03	fibr_ter_05	fibr_ter_06	fibr_ter_07	fibr_ter_08	GIPO_K	K_BLOOD	GIPER_NA	NA_BLOOD	ALT_BLOOD	AST_BLOOD	KFK_BLOOD	L_BLOOD	ROE	TIME_B_S	R_AB_1_n	R_AB_2_n	R_AB_3_n	NA_KB	NOT_NA_KB	LID_KB	NITR_S	NA_R_1_n	NA_R_2_n	NA_R_3_n	NOT_NA_1_n	NOT_NA_2_n	NOT_NA_3_n	LID_S_n	B_BLOK_S_n	ANT_CA_S_n	GEPAR_S_n	ASP_S_n	TIKL_S_n	TRENT_S_n	FIBR_PREDS	PREDS_TAH	JELUD_TAH	FIBR_JELUD	A_V_BLOK	OTEK_LANC	RAZRIV	DRESSLER	ZSN	REC_IM	P_IM_STEN	LET_IS
```

## Stage 2: Descriptive Analysis

### EDA

```{r}
#library(GGally)
#Problem is how to deal with missing data before do all pairs
#ggpairs(mi_comp,upper = list(continuous = wrap("cor", size = 2.5)),lower = list(continuous = "smooth"))
#also based on Descriptive Statistics.pdf, the Chronic heart failure (ZSN) has highest prob of complication!!! (focus on this response var most?!)
```
- How to split data that one part to train and another do what? ANSWER: LASSO - see Hughes-Oliver 7/12/2022 Notes & Lab!
- This is where visualization happens
- Remember dealing with missing data.
- Rare complication, rare event â†’ THEN less likely to be a useful reponse varible????
- Think for some BOMB Visualization
```{r}
#cleaning data
mi_compNEW.clean = na.omit(mi_compNEW); dim(mi_compNEW.clean) #0 observation????
```
```{r}
#find which variable that has a LOT of missing values to delete so above won't be 0 abs.
for (i in names(mi_compNEW)){
  print(i)
  print(sum(is.na(mi_compNEW[i])))
}
```
```{r}
#subset the dataset to delete those columns with a lot of NAs
drop <- c("IBS_NASL", "S_AD_KBRIG", "D_AD_KBRIG","KFK_BLOOD")
mi_compNEW.subset = mi_compNEW[, !(names(mi_compNEW) %in% drop)]

dim(mi_compNEW.subset) #NOW: 1700  120
```


```{r}
#Further modify NA of the dataset:
mi_compNEW.subset2 = na.omit(mi_compNEW.subset)
dim(mi_compNEW.subset2)#still deleting TOO many rows :(

#Look further how to deal with missing data:
#https://towardsdatascience.com/smart-handling-of-missing-data-in-r-6425f8a559f2
##########FIND HOW MANY MAXIMUM EXPLANATORY VAR MICE CAN TAKE!!
```
#### More exploration of missing data
```{r}
library(naniar)
# Which variables contain the most missing variables?
gg_miss_var(mi_compNEW)
# Where are missings located?
vis_miss(mi_compNEW) + theme(axis.text.x = element_text(angle=80))
# Which combinations of variables occur to be missing together?
gg_miss_upset(mi_compNEW)
```
```{r}
# ANSWER: multiple imputation by chained equations (MICE) is flexible b/c use remaining info in data to create a model that predicts what could have been recorded to fill in blanks. Fill in the missing values multiple times and create several complete datasets before we pool the results to arrive at more realistic results. 
library(mice)

#save variables that are at least somewhat correlated with r>0.25 with it.
pred_mat <- quickpred(mi_compNEW, mincor = 0.25)
```
```{r}
#run actual imputation procedure 10 times, set a seed, select a method and use the prediction matrix on our original dataset.

#mice
nhanes_multimp <- mice(mi_compNEW, m=10,meth='pmm', seed = 5, predictorMatrix = pred_mat)

#Run regresson each of the 10 imputed datasets and pool the results in the end. PROBLEM: Don't know what model to use?! In a loop: missing data then model, model then missing data????


```
MICE follow another website. (MAR)
```{r}
library(mice)
init = mice(dat, maxit)
```


#### Create train and test sets
```{r}
##split data into training set and testing set
#install.packages("caret")
set.seed(777) #MAKE RESULT REPRODUCABLE!!! MUST INCLUDE!!!
inTrain <- as.vector(
           caret::createDataPartition(mi_compNEW.subset[,1], p = 0.8, list = FALSE, times = 1 ) )
#dTrain <- mi_compNEW.subset[inTrain,]; dim(dTrain)
#dTest <- mi_compNEW.subset[-inTrain,]; dim(dTest)

#[1] 1700  120
#[1]   0 120
```
#### Prepare the data for use in glmnet
```{r}

```
- Big idea: Want to do 12 models (12 response var) of Logistic LASSO with ALL explanatory variables. "binomial"
- FOR LOOP
- Create a list to store answers.


## Stage 3: Inferential Analysis
```{r age&heart falure (ZSN)}
ageZSN <- lm(ZSN ~ AGE, na.action=na.omit, data=mi_comp)
summary(ageZSN)
```


### Logistic Regression/MLR
NO LOGISTIC FOR SURE. B/C ALL COMPLICATION(RESPONCE VAR) ARE CATEGORICAL/BINARY VARIABLES


- Think for some Q&A question that people might ask
- 